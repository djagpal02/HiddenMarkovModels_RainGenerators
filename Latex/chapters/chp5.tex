Two models can use exteremly different technqiues to predict the same thing. Thus being able to compare these models is vital. Even if the models are the same, for parameter estimation, we must compare the Performance of the model with multiple sets of paramters. To do so we need a metric that we can compare accross all models and parameter estimations. This metric is given by the likelihood function.

\begin{definition} Likelihood Function \\
    Given model $\theta$ and the observation $x_1, x_2,...,x_n$, the likelihood function is given by
     \begin{equation}
        \label{likelihood}
        L(\theta|x_1, x_2,...,x_n) = \prob(x_1, x_2,...,x_n|\theta)
     \end{equation} 
     In other words it is given by the probability that the given model produced the given observations.
\end{definition}

This function allows us to directly compare which model has a higher probability of fitting the system, based on the data. To do so we use Maximum likelihood estiamtion.


\section{Maximum Liklihood Estimators}

Through maximizing the likelihood function \ref{likelihood}, we can find the model that best fits the given observations. This process is called Maximum Likelihood estimation.

We often use log$(L(\theta|x_1, x_2,...,x_n))$ instead of $L(\theta|x_1, x_2,...,x_n)$ as this often leads to simplier differenciation. This is possible since the log function is monotonically increassing thus they have their maximum at the same value of $\theta$.

The method can be described as follows:
\begin{enumerate}[i]
    \item Calculate the joint probability density of observeriving your data $X_1 = x_1,...,X_n=x_n$ given your model $\theta$, i.e.
    \begin{equation}
        \prob(X_1 = x_1,...,X_n=x_n|\theta)
    \end{equation}
    \item Take the Natural logarithm of both sides. 
    \begin{equation}
        \log(\prob(X_1 = x_1,...,X_n=x_n|\theta))
    \end{equation}
    \item Take the partial derivative with respect to each parameter.
    \item For each paramter, set the derivative equal to 0 and rearrange for its value. The given value will be the maximum likelihood estimation of that parameter.
\end{enumerate}

Unfortunately in real world applications the derivative of the log-Liklihood is analytically intractable. This is particurally true for Hidden Markov based models. Thus we often require alternative methods to compare models.


\section{Approximate Bayesian Computation}

Approximate Bayesian Computation (ABC) is an alternative method of parameter estiamtion and model Selection. Using Bayesian inference, we calcuate the posterior distribution, one containing information from botht the data and the prior distribtuion. 

We uses Bayes original theorem applied to point probabilities and observations $y$ to obtain the following:
\begin{equation}
    \pi(\theta | y) \propto \pi(\theta) f(y|\theta)
\end{equation} 
where $\pi(\theta | y)$ is the probability distribtuion of the paramters given the data (posterior distribtuion), $\pi(\theta)$ is the probability distribtuion of the paramters before we take into account the evidence (prior distribtuion) and $f(y|\theta)$ is the likelihood.

Instead of calculating the probability directly, using some preset parameters ABC creates a simulation and then compares this to the observation dataset. The method can be described as follows:
**********REFFERENCE******************

If N iterations are desired for i=1 to N:
\begin{enumerate}
    \item Generate simulation $\theta '$ from the prior distribution $\pi(\text{data})$.
    \item Generate Z (new value for parameter/s) from the likelihood $f(\text{data}|\theta '$)
    \item If distance between statistic $\eta$ of $y$ and $z$ is less than or equal to some $\epsilon$ set $\theta_i = \theta'$ else repeat until condition is fullfilled.
\end{enumerate}

Given a small enough tolerence ($\epsilon$) and appropriate statistic $\eta$ we have:
\begin{equation}
    \label{approxpi}
    \pi_{\epsilon}(\theta|y) = \int \pi_{epsilon}(\theta, z|y)dz \approx \pi(\theta|y)
\end{equation}
as the actual posterior distribtuion is the limit of \ref{approxpi} as $\epsilon$ goes to 0.


\section{Kolmogorov–Smirnov test}

Kolmogorov–Smirnov test
