%\label{Generalising_the_Observation_Matrix}

Our HMM has an extremely large observation matrix. In this chapter, we aim to simplify this to a function of random variables that defines the whole matrix with only a handful of parameters and then solve for these parameters. We take inspiration from ideas presented by Cowperthwait and Grando.

\section{Generalised Model}
\label{Generalising_the_Observation_Matrix:Generalised_Model}

We have seen that HMM can simulate rainfall similar to that given in the training data. While this is useful, it also has some limitations. For example, due to our discretised system with integer value groups, some groups have 0 probability. The data justifies this result, but it cannot be explained by nature. For example, for our selected attempt of 1 in month 0, given the system is in state 1, the probability of 1mm of rainfall is 0.0284927; however, 2mm is 0. This result may be accurate for the data but is not a reasonable assumption in nature. As such, we must find a method to generalise the model. Generalising the observation matrix such that the simulation represents the training data and helps simulate future rainfall patterns.


\section{Function for Observation Matrix}
\label{Generalising_the_Observation_Matrix:Function_For_Observation_Matrix}

To generalise our model, we require a function for each row of the observation matrix. We could test for polynomial fits, but we will attempt to use our model to have real-life justifications. We propose our model, which was developed by adjusting Grando's Model.

\begin{prop}
    \label{model}
    Given the system is in state i, the amount of rainfall in mm is given by
    \begin{equation}
        \sum_{i=0}^N \sigma_i
    \end{equation}
    where $N \sim pois(\lambda)$ is the number of storm discs and $\sigma \sim exp(\tau)$ is the intensity of rain in each.
\end{prop}



\section{Parameter Estimation}
\label{Generalising_the_Observation_Matrix:Parameter_Estimation}

Now that we have a function, in this section, we estimate our parameters.

According to \ref{model}, we have two unknown parameters for each row of each observation matrix. In other words, for our example, we have six unknown parameters for each month. Fortunately, we do not have to attempt to estimate all of these from the same data. We can use the distribution given by each row to generate a simulation and then attempt to fit our two parameters to this.


    \subsection{Methodology}
    \label{Generalising_the_Observation_Matrix:Parameter_Estimation:Methodology}

    The programs used for this are "pe" and "Analysis.R" both in the parameter estimation folder. The program "pe" follows the following steps:
    \begin{enumerate}
        \item Input of the minimum and maximum value desired of $\lambda$ and $\tau$ as well as which row is being fit.
        \item Uniformly randomly selects values within the given ranges and generate a simulation of size 100000 for the given row.
        \item Create array containing frequency counts of each integer from simulation.
        \item Using observation matrix row as a distribution, find the expected frequency for 100000 simulations by multiplying the probability by the number of iterations. Store results in an array.
        \item Find the difference between each frequency of each element in the simulation and the expectation array. Sum absolute value of each and take this value as residual forgiven $\lambda$ and $\tau$. 
        \item Repeat steps 1000 times forgiven $\lambda$ and $\tau$. Record $\lambda$ and $\tau$ and residual in CSV file. 
    \end{enumerate}

    \subsection{Estimation}
    \label{Generalising_the_Observation_Matrix:Parameter_Estimation:Estimation}

    "pe" has been optimised to run over up to 24 threads. When given the 24 threads,  the program finishes the above computation within 10-30 seconds.


    "Analysis.R" is used to analyse the CSV file. It plots three graphs; a 3D graph of $\lambda$ against $\tau$ against residual, $\lambda$ against residual and $\tau$ against residual. We use these three graphs to find an approximate area of $\lambda$ or $\tau$ where residual is lowest and then repeat "pe" for this new area. 

    We will demonstrate the results for the first row of the observation matrix for Month 0. The process followed for calculating all other parameter estimates has been the same. 

    \begin{figure}
        \begin{subfigure}{.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam0-10tau0-10.png}
        \caption{x = $\lambda$, y = $\tau$, z = Residual}
        \label{pe1:1}
        \end{subfigure}

        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam_1.png}
        \caption{}
        \label{pe1:2}
        \end{subfigure}
        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/tau_1.png}
        \caption{}
        \label{pe1:3}
        \end{subfigure}
        \caption{First iteration of parmeter estimation. Parameters set to: $\lambda$ between 0 and 10, $\tau$ between 0 and 10. Graphs show clear drop in residual where $\lambda \in [0,2]$ and $\tau \in [0,0.5]$.}
        \label{pe1}
    \end{figure}

    \begin{figure}
        \begin{subfigure}{.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam0-2tau0-0_5.png}
        \caption{x = $\lambda$, y = $\tau$, z = Residual}
        \label{pe2:1}
        \end{subfigure}

        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam_2.png}
        \caption{}
        \label{pe2:2}
        \end{subfigure}
        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/tau_2.png}
        \caption{}
        \label{pe2:3}
        \end{subfigure}
        \caption{First iteration of parmeter estimation. Parameters set to: $\lambda$ between 0 and 2, $\tau$ between 0 and 0.5. Graphs show clear drop in residual where $\lambda \in [0.5,0.75]$ and $\tau \in [0,0.1]$.}
        \label{pe2}
    \end{figure}

    \begin{figure}
        \begin{subfigure}{.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam0_5-0_75tau0-0_1.png}
        \caption{x = $\lambda$, y = $\tau$, z = Residual}
        \label{pe3:1}
        \end{subfigure}

        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/lam_3.png}
        \caption{}
        \label{pe3:2}
        \end{subfigure}
        \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Parameter Estimation on Attempt _1/param_1/tau_3.png}
        \caption{}
        \label{pe3:3}
        \end{subfigure}
        \caption{First iteration of parmeter estimation. Parameters set to: $\lambda$ between 0.5 and 0.75, $\tau$ between 0 and 0.1. Graphs show minimum $\lambda$ at around 0.625 and minimum $\tau$ at around 0.05.}
        \label{pe3}
    \end{figure}

    We begin by running "pe" between 0-10 for both tau and lambda. We found that both values tend to have their minimum residual points within these ranges through initial testing with random ranges. It is clear from Figure \ref{pe1} that $\lambda \in [0,2]$ and $\tau \in [0,0.5]$. Thus we repeat the program for this new range. The results can be seen in figure \ref{pe2}. This time we can see that $\lambda \in [0.5,0.75]$ and $\tau \in [0,0.1]$. We repeat the program one more time with these new ranges and retrieve Figure \ref{pe3}. We can now approximate that $\lambda \approx 0.625$ and $\tau \approx 0.05$. 

    We repeat this process for every row of the observation matrix for every month. The results are as given:

    \begin{center}
        \begin{tabular}{c c c c c c c}
            \label{petable}
            Month   &   $\lambda_1$ &   $\lambda_2$ &   $\lambda_3$ &   $\tau_1$    &   $\tau_2$    &   $\tau_3$   \\
                0   &   0.625       &   0.9         &   0.775       &   0.05    &   0.2     &   0.06    \\
                1   &   0.265       &   3.25        &   1.95        &   0.06    &   0.15    &   0.055   \\
                2   &   0.42        &   3.3         &   4           &   0.09    &   0.11    &   0.1     \\
                3   &   2.5         &   2.5         &   0.35        &   0.12    &   0.175   &   0.06    \\
                4   &   2.3         &   3.25        &   0.525       &   0.09    &   0.16    &   0.075   \\
                5   &   4           &   1.55        &   0.3         &   0.13    &   0.1     &   0.06    \\
                6   &   2.25        &   0.6         &   0.55        &   0.075   &   0.075   &   0.075   \\
                7   &   3.4         &   0.24        &   2.15        &   0.2     &   0.075   &   0.1     \\
                8   &   0.57        &   0.52        &   3.2         &   0.04    &   0.06    &   0.12    \\
                9   &   5.5         &   0.3         &   2.6         &   0.17    &   0.05    &   0.25    \\
                10  &   0.27        &   9           &   2.75        &   0.05    &   0.64    &   0.075   \\
                11  &   4.15        &   0.24        &   2           &   0.1     &   0.075   &   0.11    

        \end{tabular}
    \end{center}

    We can see from the table that while the values of $\tau$ are often similar, lambda values vary drastically. Regardless, we are concerned with the performance of this model. In the next chapter, we test this model and apply more rigorous tests on the hmm only model.




