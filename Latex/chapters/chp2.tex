%\label{Hidden_Markov}

Before we try to apply Hidden Markov Models (HMMs) to rainfall, we must first understand how they work. In this chapter we will build an understanding for the basic principles behind HMMs, along with developing and awnsering a few key questions.

\section{Definition}
\label{Hidden_Markov:Definition}

In this section we will define a HMM and introduce notation that will be used throughout this paper. 

A hidden Markov model is a doubly stochastic process, where the underlying process is Markovian and hidden; \cite{Rabiner1986}. This comes from the fact that there are two stochastic processes, one determining the transition between states and one determining the output observation. Braum and his colleagues developed this in \cite{Baum1967} and \cite{Baum1966}. 

To define a HMM, we require five things. These are as follows:
To define a hidden Markov model, we need five things.

\begin{enumerate}
    \item N \begin{enumerate}[i]
        \item N is the number of hidden states. Usually based on something in the real world but sometimes can be unknown.
        \item The states are usually ergodic, i.e. from any given state, we can eventually reach all others, but this is not necessary.
        \item The states are from the state space $S = \{S_1,S_2,...,S_N\}$.
        \end{enumerate}
    \item M \begin{enumerate}[i]
        \item M is the number of observable outputs.
        \item These make a discrete alphabet of observations called $V = \{v_1, v_2,...,v_M\}$.
    \end{enumerate}
    \item A \begin{enumerate}[i]
        \item A is the state transition matrix.
        \item This is the same as the $p$ matrix for simple Markov chains.
        \item A = $\{a_{ij}\}$ 
        \item $a_{ij}$ = $\{ p(i,j) = \prob \{ X_n = j | X_{n-1}\}$
    \end{enumerate}
    \item B \begin{enumerate}[i]
        \item B is the observation probability matrix.
        \item B = $\{b_j(k)\}$
        \item $b_j(k) = \prob (V_k \  at \  t | q_t = S_j)_{1 \leq j \leq N, 1 \leq k \leq M}$
    \end{enumerate}
    \item $\pi$ \begin{enumerate}[i]
        \item $\pi$ is a vector containing all inital state probabilities. 
        \item $\pi$ = $\{ \pi_i \}$
        \item $\pi_i$ = $\prob (q_1 = S_i)$ for $1 \leq i \leq N$
    \end{enumerate}
\end{enumerate}

We can now combine the above to provide a formal definition of HMMs.

\begin{definition}
    \label{Definition:HMM} Hidden Markov Model \\
    A Hidden Markov Model is a 5-tuple $\{N,M,A,B,\pi \}$ that is used to represent a doubly stochastic process where the hidden process is Markovian. 
\end{definition}

Before we continue, we will also provide some more notation, which will be used for the rest of this paper.
\begin{enumerate}[i]
    \item $Q$ represents the Markov model's state sequence, i.e. the hidden process.
    \item We will use $O = \{o_1,o_2,...,o_T\} ,\ O_i \in V$ to represent the observation sequence.
    \item we will occasionally represent the hidden markov model as $\{N,M, \lambda \}$ where $\lambda = \{A, B, \pi\}$ 
\end{enumerate}


\section{Using HMM}
\label{Hidden_Markov:Using_HMM}

As with all models, we can use HMM to simulate processes. In this section we will demonstrate how to do so for a HMM with known parameters as well as propose a few questions one may ask.

    \subsection{Simulation}
    \label{Hidden_Markov:Using_HMM:Simulation}

    Simulating a process is usually our reason for building a model.   Thus, in this subsection, we will demonstrate how to simulate a HMM to generate a observation sequence.

    Let H = $\{N,M,A,B,\pi\}$ be a HMM. To generate a sequence of $T$ observations $O$ we do the following steps:
     \begin{enumerate}
        \label{Hidden_Markov:Using_HMM:Simulation:Method}
         \item Using $\pi$ as a probability measure, set $t$ = 1 and    randomly sample a state as the first $q_1$ = $s_i$.
         \item Using $b_i(k)$ as a probability measure, randomly sample the observation $O_t$ = $v_k$.
         \item Using row $i$ of $a_{ij}$ as a probability measures, set $t$ = $t+1$    and randomly sample a the next state $q_{t+1}$ = $s_j$.
         \item Repeat steps 2 and 3 until $t$ = $T$
     \end{enumerate}


     \begin{example}
        \label{Hidden_Markov:Using_HMM:Simulation:Example}
        Suppose Alice is hidden away from the world and has no access to information regarding the weather. She meets Bob every day and knows how weather affects his mood.
        For simplicity, assume Bob only has two moods, happy and sad.   Given matrix A, B and vector $\pi$ can she predict his mood for   three consecutive days?
        \begin{figure}
            \centering
            \begin{tikzpicture}
                \node[state] at(-3,0) (s) {Sunny};
                \node[state] at(3,0)  (r) {Rainy};
                \node[state] at(0,-3)  (c) {Cloudy};
                \node at (-6,3) (h1) {Happy};
                \node at (-6,-3) (s1) {Sad};
                \node at (3,-6) (h2) {Happy};
                \node at (-3,-6) (s2) {Sad};
                \node at (6,3) (h3) {Happy};
                \node at (6,-3) (s3) {Sad};

                \draw[every loop]
                    (s) edge[bend right=20, auto=left] node {0.4} (r)
                    (s) edge[bend right=20, auto=right] node {0.2} (c)
                    (s) edge[loop left] node {0.4} (s)       
                    (r) edge[bend right=20, auto=left] node {0.4} (c)
                    (r) edge[bend right=20, auto=right] node {0.3} (s)
                    (r) edge[loop right] node {0.3} (r) 
                    (c) edge[bend right=20, auto=right] node {0.5} (r)
                    (c) edge[bend right=20, auto=left] node {0.2} (s)
                    (c) edge[loop below] node {0.3} (c);

                \draw [->, dashed] (s) edge[auto=right] node{0.2} (s1);
                \draw [->, dashed] (s) edge[auto=right] node{0.8} (h1);
                \draw [->, dashed] (c) edge[auto=right] node{0.3} (s2);
                \draw [->, dashed] (c) edge[auto=right] node{0.6} (h2);
                \draw [->, dashed] (r) edge[auto=right] node{0.3} (s3);
                \draw [->, dashed] (r) edge[auto=right] node{0.7} (h3);
            \end{tikzpicture}
            \label{Hidden_Markov:Using_HMM:Simulation:Example_Figure}
            \caption{A chain representing the HMM with hidden states the weather and observable states mood.The solid lines represent transitions between hidden states, and the dashed represent observable.}
        \end{figure}
       
        From context we can deduce the following:
        \begin{center}
                    N = 3, number of hidden states

                    M = 2, number of possible observations

                    \begin{equation}
                    A = 
                    \begin{array}{c}
                        (Sunny) \\
                        (Rainy) \\
                        (Cloudy)
                    \end{array}
                    \begin{bmatrix}
                        0.4 & 0.4 & 0.2 \\
                        0.3 & 0.3 & 0.4 \\
                        0.2 & 0.5 & 0.3 
                        \end{bmatrix}
                    \end{equation}

                    \begin{equation}
                    B = 
                    \begin{array}{c}
                        (Sunny) \\
                        (Rainy) \\
                        (Cloudy)
                    \end{array}
                    \begin{bmatrix}
                        0.8 & 0.2 \\
                        0.7 & 0.3 \\
                        0.6 & 0.4 
                        \end{bmatrix}
                    \end{equation}

                    \begin{equation}
                    \pi = 
                    \begin{array}{c}
                        (Sunny) \\
                        (Rainy) \\
                        (Cloudy)
                    \end{array}
                    \begin{bmatrix}
                        0.5 \\
                        0.3 \\
                        0.2 
                        \end{bmatrix}
                    \end{equation}
        \end{center}

        We can now use \ref{Hidden_Markov:Using_HMM:Simulation:Method} to create an observation sequence   $O$ = $\{o_1, o_2, o_3\}$. We will require multiple random variables generated using various probability measures. We will generate uniform random variabes through python using the "rand.py" file. We will then combine these with inversion sampling to select the state.

        \begin{enumerate}[i]
            \item We generate a r.v. = 0.0058. Using Pi as the probaility   distribution, we select Sunny. We can now set $q_1$ = $s_1$.
            \item We generate a r.v. = 0.1947. Using $b_1(k)$ as the    probability distribution we select Happy as the observation.   We can now set $o_1$ = $v_1$.
            \item we generate a r.v. = 0.7168. Using $a_{1j}$ as the    probability distribution we select Rainy as the next state.    We now set $q_2$ = $s_2$.
            \item We generate a r.v. = 0.1060. Using $b_2(k)$ as the    probability distribution we select Happy as the observation.   We can now set $o_2$ = $v_1$.
            \item we generate a r.v. = 0.8977. Using $a_{2j}$ as the    probability distribution we select Cloudy as the next state.   We now set $q_3$ = $s_3$.
            \item We generate a r.v. = 0.1369. Using $b_3(k)$ as the    probability distribution we select Happy as the observation.   We can now set $o_2$ = $v_1$.
        \end{enumerate}
        Finally, we can look back on our simulated observations $O$ and see that it is equal to $\{v_1,v_1,v_1\}$, i.e. we observe three consecutive Happy days.
    \end{example}


    \subsection{Key Problems}
    \label{Hidden_Markov:Using_HMM:Key_Problems}

    To ensure the validity of a model, one may ask questions regarding the probabilities of certain occurances. We discuss these questions in this subsection.

    We will focus on three famous questions highlighted in  \cite{Rabiner1986}. 
    \begin{enumerate}
        \label{Hidden_Markov:Using_HMM:Key_Problems:Questions}
        \item \label{Hidden_Markov:Using_HMM:Key_Problems:Questions:First} Evaluation \\ Given model H = $\{N,M,A,B, \pi\}$ what is the probability that it generated the sequence of observations $O = \{o_1,o_2,...,o_T\}$? i.e. $\prob (O \ | \ H)$
        \item \label{Hidden_Markov:Using_HMM:Key_Problems:Questions:Second} Decoding \\ What sequence of states $Q = \{q_1, q_2,..., q_3\}$ best explains a sequence of observations $O  = \{o_1,o_2,...,o_T\}$?
        \item \label{Hidden_Markov:Using_HMM:Key_Problems:Questions:Third} Learning \\ Given a set of observation $O =   \{o_1,o_2,...,o_T\}$, how can we learn the model H = $\{N,M,A,B,  \pi\}$ that would generate them?
    \end{enumerate}
    
    In the coming sections, we will be motivating uses and developing  solutions for each problem. 


\section{Problem 1: Evaluation}
\label{Hidden_Markov:Evaluation}

In this section we will tackle the Evaluation problem for HMM; \ref{Hidden_Markov:Using_HMM:Key_Problems:Questions:First}. We will first attempt using a simple approach and then find a more efficient implementation; the Forward-Backward Algorithm.

We are looking for the probability that a given model generated a sequence of observations, i.e. $\prob (O | \lambda)$. This probability has many useful applications. For example, we may have multiple potential models $\lambda_i$ and cannot decide which one is  most suitable. We can calculate this probability for each $\lambda_i$ and the select the one with the largest.

To find this probability, we must consider the hidden internal states of the model. Since our probability of observations $\{b_j(k)\}$ is conditioned on the hidden state, we can start by calculating this probability conditioned on these states. Let us assume we have the state sequence $Q = \{q_1,q_2,...,q_t\}$. For $\prob (O | Q, \lambda)$ we can find the product of all the probabilities of an observation given the models state at all times $t$. In essence, this breaks the  $\prob (O | Q, \lambda)$ into T parts. 
\begin{equation}
    \label{Hidden_Markov:Evaluation:Equation1}
    \prob (O | Q, \lambda) = \prod_{t=1}^T \prob (O_t | q_t, \lambda)
\end{equation}


One may observe that these probabilities are simply taken from the matrix $B$. 
\begin{equation}
    \prob (O_t | q_t, \lambda) = b_{q_t}(O_t), \ \ \forall t \in [0,T] 
\end{equation}

Thus we can rewrite \ref{Hidden_Markov:Evaluation:Equation1} as:
\begin{equation}
    \label{Hidden_Markov:Evaluation:Equation2}
    \prob (O | Q, \lambda) = b_{q_1}(O_1) b_{q_2}(O_2)  ...  b_{q_T}(O_T)
\end{equation}

Our next objective is to remove $Q$ from the conditioned part of the probability. To do this, we must first calculate $\prob (Q | \lambda)$. This is simply the probability of transitioning from $q_1$ to $q_2$, $q_2$ to $q_3$ etc. More formally we can use matrix $A$ to find the probability of each of these transitions, and since we are finding the total for the entire sequence, we multiply them all together. We start with $\pi_{q_1}$ as we also need the probability of starting at $q_1$.
\begin{equation}
    \label{Hidden_Markov:Evaluation:Equation3_InQandLambda}
    \prob ( Q | \lambda) = \pi_{q_1} a_{q_1q_2} a_{q_2q_3} ... a_{q_{T-1}q_T}
\end{equation}

We can now successfully remove $Q$ from the condition using \ref{Hidden_Markov:Evaluation:Equation2} and \ref{Hidden_Markov:Evaluation:Equation3_InQandLambda}:
\begin{eqnarray}
    \label{Hidden_Markov:Evaluation:Equation_oqgivenlam}
    \prob (O,Q | \lambda) & = & \prob(O | Q, \lambda) \prob(Q | \lambda) \\
                          & = & \pi_{q_1} b_{q_1}(O_1) a_{q_1q_2} b_{q_2}(O_2) a_{q_2q_3} ... a_{q_{T-1}q_T} b_{q_T}(O_T)
\end{eqnarray}

This is the joint probability of observations and the internal states. In other words, it provides the probabilty that given observations $O$ and internal state sequence $Q$ were generated by model $\lambda$. To achieve our desired probablity all we need to do is get rid of the $Q$. Since it is another input, all we must do is sum each value of \ref{Hidden_Markov:Evaluation:Equation_oqgivenlam}. As we have accounted for every possible $Q$, we no longer need to worry about its particular value. This leaves us with:
\begin{eqnarray}
    \label{Hidden_Markov:Evaluation:Equation_ogivenlamsumq}
    \prob (O | \lambda) & = & \sum_{all Q} \prob(O| Q,\lambda)P(Q|\lambda) \\
                        & = & \sum_{q_1,q_2,...,q_T} \pi_{q_1} b_{q_1}(O_1) a_{q_1q_2} b_{q_2}(O_2) a_{q_2q_3} ... a_{q_{T-1}q_T} b_{q_T}(O_T)
\end{eqnarray} 

Unforuntately calculating this is infeasible as it requires too many computations. For $T$ timesteps and $N$ states, to find every possible $Q$, we must sum over $N^T$ state sequences. For each timestep we require a multiplication to $a_{q_{i-1}q_{i}}$ and $b_{q_i}(O_i)$, except the last where there are no transitions, which leads to $2T-1$ multiplications for each state sequence. Lastly, we require $N^T$ addition operations to sum the result for each state sequence. Our final total number of operations of $(2T-1)N^T + (N^T-1)$. 

\begin{example}
    \label{Hidden_Markov:Evaluation:motivfb}
    Suppose we would like to find the probability of a string of 10 observations using model H, where $N$ = 3 and $T$ = 10. 
    \begin{eqnarray}
        operations & = & (2T-1)N^T + (N^T-1) \\
                   & = & (20-1)3^10 + (3^10 -1) \\
                   & = & 1180979
    \end{eqnarray}
\end{example}


We have a problem as even with a small model we require a considerable number of calculations. For a moderate to large-sized model, we would require an infeasible amount of calculations. To overcome this problem, we look for a more efficient method.



    \subsection{Forward-Backward Algorithm}
    \label{Hidden_Markov:Evaluation:Forward_Backward_Algorithm}

    The Forward-Backward (FB) algorithm offers an alternative, more efficient, method to find $\prob (O \ | \ H)$. In this subsection we will demonstrate its use. Our explanations take motivation from \cite{Rabiner1986}.

    To use FB, we must first introduce two helper functions $\alpha$ and $\beta$. We will start by discussing the former. 

    \begin{equation}
        \label{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha}
        \alpha_t(i) = \prob (O_1,O_2,O_3,...,O_t,q_t=S_i | \lambda)
    \end{equation}
    $\alpha$ is an extremly powerful tool in reducing the number of calculations. As given by \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha}, it providse the probability that at time $t$ we have seen a sequence of observations and are currently at state $q_t=S_i$. This is not quite $\prob (O | \lambda)$ but it represents a part of it. Instead of the probability of the whole sequence, it breaks it into a chunk of size $t$, commits to end at a particular state and then calculates the same proabbility for this.

    We can combine \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha} with induction to produce an iterative process that can solve our problem.

    \begin{enumerate}[i]
        \item Base case \\
        For the base case we require the probability of the $q_1$ being equal to $S_1$ and giving us observation $O_1$. The former is addressed by $\pi_i$ and the later by $b_i(O_1)$. This gives us:
        \begin{eqnarray}
            \alpha_1(i) = \pi_i b_i(O_1), & i \in [1,N]
        \end{eqnarray}

        \item Inductive step:
        \begin{figure}
            \begin{center}
                \begin{tikzpicture}
                    \node[state] at(-2,3) (s1) {$S_1$};
                    \node[state] at(-2,1)  (s2) {$S_2$};
                    \node at(-2,0)  (sd1) {.};
                    \node at(-2,-0.5)  (sd2) {.};
                    \node at(-2,-1) (sd3) {.};
                    \node[state] at(-2,-2)  (sn) {$S_N$};
                    \node[state] at(3,0) (new) {$S_j$};
                    \node at(-2,-3) (a1) {$\alpha_t(i)$};
                    \node at(3,-3) (a2) {$\alpha_{t+1}(i)$};
                    \node at(-2,-4) (t1) {$t$};
                    \node at(3,-4) (t2) {$t+1$};
    
                    \draw [->] (s1) edge[auto=right] node{$a_{1j}$} (new);
                    \draw [->] (s2) edge[auto=right] node{$a_{2j}$} (new);
                    \draw [->] (sn) edge[auto=right] node{$a_{Nj}$} (new);
                \end{tikzpicture}
                \label{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Figure_Alpha}
                \caption{For each state $j$, for each inductive step, we follow the above method. Each$\alpha_t  (i)$ is multiplied by each $a_{ij}$ and then summed up. This resultmultipled by $b_j  (O_{t+1}$ gives us $\alpha_{t+1}(j)$.}
            \end{center}
        \end{figure}

        For the inductive step we must consider how to approach the next timestep. We will again be calculating for all $j\in [1,N]$ and as such must take into consideration, for each $j$, every possible $i$. This is again the same set of $[1,N]$. Therefore, to account for all possible previous states and their transition to the current state, we must sum over 1 to $N$ the product of $\alpha_t(i)$ and $a_{ij}$. For the given observation, as before, we compute $b_j(O_{t+1})$. Additionally, we must stop before reaching the final step as there is no outward transition and thus this would not be applicable. This gives us:
        \begin{eqnarray}
            \alpha_{t+1}(j) = \left[\sum_{i=1}^N \alpha_t(i)a_{ij}\right] b_j(O_{t+1}), & j \in [1,N], t \in [1,T-1]
        \end{eqnarray}

        \item Termination step: \\
        For the termination step, we sum over all alpha at the final time $T$. Each alpha represents the probability of being in that given state at that particular time, through all possible sequences that it may have followed. 
        \begin{eqnarray}
            \prob (O | \lambda) = \sum_{i=1}^N \alpha_T(i)
        \end{eqnarray}
    \end{enumerate}


    \begin{figure}
        \begin{center}
            \begin{tikzpicture}
                \node[state] at(-3,3) (a11) {$\alpha_1(1)$};
                \node[state] at(-3,1)  (a12) {$\alpha_1(2)$};
                \node at(-3,0)  (sd1) {.};
                \node at(-3,-0.5)  (sd2) {.};
                \node at(-3,-1) (sd3) {.};
                \node[state] at(-3,-2)  (a1n) {$\alpha_1(N)$};
                
                \node[state] at(0,3) (a21) {$\alpha_2(1)$};
                \node[state] at(0,1)  (a22) {$\alpha_2(2)$};
                \node at(0,0)  (sd12) {.};
                \node at(0,-0.5)  (sd22) {.};
                \node at(0,-1) (sd32) {.};
                \node[state] at(0,-2)  (a2n) {$\alpha_2(N)$};
                \node[state] at(3,3) (a31) {$\alpha_{T-1}(1)$};
                \node[state] at(3,1)  (a32) {$\alpha_{T-1}(2)$};
                \node at(3,0)  (sd13) {.};
                \node at(3,-0.5)  (sd23) {.};
                \node at(3,-1) (sd33) {.};
                \node[state] at(3,-2)  (a3n) {$\alpha_{T-1}(N)$};
                
                \node[state] at(6,3) (a41) {$\alpha_{T}(1)$};
                \node[state] at(6,1)  (a42) {$\alpha_{T}(2)$};
                \node at(6,0)  (sd14) {.};
                \node at(6,-0.5)  (sd24) {.};
                \node at(6,-1) (sd34) {.};
                \node[state] at(6,-2)  (a4n) {$\alpha_{T}(N)$};
                \node at(-3,-4) (1) {1};
                \node at(6,-4) (T) {T};
                \node at(-5,-2) (N) {N};
                \node at(-5,3) (1state) {1};
                \node at(1.5,-4.5) (obs) {Observations};
                \node at(-6, 0.5) (State) {States};
                \draw (1) -- (T);
                \draw (1state) -- (N);
                \draw [->] (a11) -- (a21);
                \draw [->] (a11) -- (a22);
                \draw [->] (a11) -- (a2n);
                \draw [->] (a12) -- (a21);
                \draw [->] (a12) -- (a22);
                \draw [->] (a12) -- (a2n);
                \draw [->] (a1n) -- (a21);
                \draw [->] (a1n) -- (a22);
                \draw [->] (a1n) -- (a2n);
                \draw [->, dashed] (a21) -- (a31);
                \draw [->, dashed] (a21) -- (a32);
                \draw [->, dashed] (a21) -- (a3n);
                \draw [->, dashed] (a22) -- (a31);
                \draw [->, dashed] (a22) -- (a32);
                \draw [->, dashed] (a22) -- (a3n);
                \draw [->, dashed] (a2n) -- (a31);
                \draw [->, dashed] (a2n) -- (a32);
                \draw [->, dashed] (a2n) -- (a3n);
                \draw [->] (a31) -- (a41);
                \draw [->] (a31) -- (a42);
                \draw [->] (a31) -- (a4n);
                \draw [->] (a32) -- (a41);
                \draw [->] (a32) -- (a42);
                \draw [->] (a32) -- (a4n);
                \draw [->] (a3n) -- (a41);
                \draw [->] (a3n) -- (a42);
                \draw [->] (a3n) -- (a4n);
            \end{tikzpicture}
            \label{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Figure_Alpha_Lattice}
            \caption{For each $\alpha$ at time $t$, we require all $\alpha$ at time $t-1$. Thus werequire $N^2$ calculations for each timestep.}
        \end{center}
    \end{figure}

    One can see that this method is far more efficient than \ref{Hidden_Markov:Evaluation:Equation_ogivenlamsumq}. It requires $N^2$ multiplications for  $\alpha_t(i)$ and $a_{ij}$  for $T$ time periods. At each $T$ there are $N$ addition operations for the summation and  $N$ multiplication for the $b_j(O_{t+1})$, which is true for all but the first and last timestep, where there are $N$ multiplications and $N$ additions respectively. This gives us $(T-2)(N^2 + N + 1) + 2N$ calculations. Considering the order, we see that the previous method had order $O(TN^T)$ whereas F-B gives us an order of $O(TN^2)$. It is now feasible to compute $\prob (O | \lambda)$.

    \begin{example}
        \label{Hidden_Markov:Evaluation:motivfb2}
        We attempt to solve the problem stated in \ref{Hidden_Markov:Evaluation:motivfb} but this time using the FB algorithm. Suppose we would like to find the probability of a sequence of 10 observations using model H. Again, $N$ = 3 and $T$ = 10. 
        \begin{eqnarray}
            operations & = & (T-2)(N^2 + N + 1) + 2N \\
                    & = & (10-2)(3^2 + 10 + 1) + 6\\
                    & = & 118
        \end{eqnarray}

        118 is many orders of magnitude smaller than 1180979, which was the requirement without FB algorithm. Thus it is a significant improvement. The improvement against the base method will be even more drastic for larger models, as FB has a fixed $N^2$ instead of the $N^T$ term.  
    \end{example}


    For question 1 this is sufficient. However, we will later require the $\beta$, backward component and as such we will describe it now. Logically it is the same principle as the forward component except this time instead of moving forward step by step we are moving backwards.
    \begin{equation}
        \label{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Beta}
        \beta_t(i) = \prob (O_{t+1},O_{t+2},,...,O_T | q_t=S_i, \lambda)
    \end{equation}
    This represents the probability of seeing the observations $O_{t+1}$ up to $O_T$ given that at time $t$ the model $\lambda$ is at state $S_i$. Once again we calculate this using induction.

    \begin{enumerate}
        \item Base case \\
        For each state we must assume that it is the final state in order to iterate from it. This gives us:
        \begin{eqnarray}
            \beta_T(i) = 1, & i \in [1,N]
        \end{eqnarray}

        \item Inductive step:
        \begin{figure}
            \begin{center}
                \begin{tikzpicture}
                    \node[state] at(3,3) (s1) {$S_1$};
                    \node[state] at(3,1)  (s2) {$S_2$};
                    \node at(3,0)  (sd1) {.};
                    \node at(3,-0.5)  (sd2) {.};
                    \node at(3,-1) (sd3) {.};
                    \node[state] at(3,-2)  (sn) {$S_N$};
                    \node[state] at(-2,0) (new) {$S_j$};
                    \node at(3,-3) (a1) {$\beta_{t+1}(i)$};
                    \node at(-2,-3) (a2) {$\beta_{t}(i)$};
                    \node at(3,-4) (t1) {$t+1$};
                    \node at(-2,-4) (t2) {$t$};

                    \draw [->] (new) edge[auto=right] node{$a_{1j}$} (s1);
                    \draw [->] (new) edge[auto=right] node{$a_{2j}$} (s2);
                    \draw [->] (new) edge[auto=right] node{$a_{Nj}$} (sn);
                \end{tikzpicture}
                \label{Hidden_Markov:Evaluation:Figure_Beta}
                \caption{Similar to $\alpha$, we use induction. However, this time we look into the future of the sequence and take steps backwards.}
            \end{center}
        \end{figure}

        Each inductive step we move back by one timestep. As before, we will be calculating for all $N$ states except this time $i$ will be varying instead of $j$. This makes sense as we are stepping backward and we want to see which previous state is the most likely previous state. At each step, the transition probability of having coming from $i$, $a_{ij}$ to the probability of seeing the given observation at state $S_j$, $b_j(O_{t+1})$ and the liklihood of being at that state based on future states $\beta_{t+1}(i)$ are multiplied. This gives us:
        \begin{eqnarray}
            \beta_{t}(i) = \sum_{j=1}^N a_{ij} b_j(O_{t+1}) \beta_{t+1}(i), & i \in [1,N], t \in [1,T-1]
        \end{eqnarray}
    \end{enumerate}


\section{Problem 2: Decoding}
\label{Hidden_Markov:Decoding}

    In this section we will address the decoding problem \ref{Hidden_Markov:Using_HMM:Key_Problems:Questions:Second}. We will use the two helper functions developed in \ref{Hidden_Markov:Evaluation} togther with the Viterbi Algorithm to do so. We will use ideas presented in \cite{Rabiner1986} and \cite{Forney1973}.

    The problem searches for the sequence of states that best explains the sequence of observations. However, the definition of 'best' is quite vague and open to interpretations. An obvious appraoch would be to look for each time $t$, find the most likely state, given all observations and the model. Lets define this probaility as $\gamma$.
    \begin{equation}
        \label{Hidden_Markov:Decoding:gamma}
        \gamma_t(i) = \prob (q_t = S_i | O, \lambda)
    \end{equation}

    Using \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha} and \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Beta} we can solve for $\gamma$ quite quickly. If we recall, $\alpha_t(i)$ provides us with the probability of being in state $i$ after seeing all the previous observations and $\beta_t(i)$ gives us the probability of being in state $i$ see all the remaining observations in the future. This suggests $\gamma$ is equal to $\alpha_t(i) \beta_t(i)$. We now normalise this to get a probability, which is possible by dividing by the probability of getting this particular observation given all possible observation sequences $\prob (O|\lambda)$. This is equivalent to dividing by the sum of $\alpha_t(j) \beta_t(j)$ over all possible states $j$.
    \begin{eqnarray}
        \gamma_t(i) & = &  \dfrac{\alpha_t(i) \beta_t(i)}{\prob (O|\lambda)} \\
                    & = &  \dfrac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)}
    \end{eqnarray}


    This ensures that:
    \begin{equation}
        \sum_{i=1}^N \gamma_t(i) = 1
    \end{equation}

    \begin{figure}
        \begin{center}
            \begin{tikzpicture}
                \node[state] at(-3,3) (a11) {$\alpha$};
                \node[state] at(-3,1)  (a12) {$\alpha$};
                \node at(-3,0)  (sd1) {.};
                \node at(-3,-0.5)  (sd2) {.};
                \node at(-3,-1) (sd3) {.};
                \node[state] at(-3,-2)  (a1n) {$\alpha$};
                
                \node[state] at(0,3) (a21) {$\alpha$};
                \node[state] at(0,1)  (a22) {$\alpha$};
                \node at(0,0)  (sd12) {.};
                \node at(0,-0.5)  (sd22) {.};
                \node at(0,-1) (sd32) {.};
                \node[state] at(0,-2)  (a2n) {$\beta$};
                \node[state] at(3,3) (a31) {$\beta$};
                \node[state] at(3,1)  (a32) {$\beta$};
                \node at(3,0)  (sd13) {.};
                \node at(3,-0.5)  (sd23) {.};
                \node at(3,-1) (sd33) {.};
                \node[state] at(3,-2)  (a3n) {$\beta$};
                
                \node[state] at(6,3) (a41) {$\beta$};
                \node[state] at(6,1)  (a42) {$\beta$};
                \node at(6,0)  (sd14) {.};
                \node at(6,-0.5)  (sd24) {.};
                \node at(6,-1) (sd34) {.};
                \node[state] at(6,-2)  (a4n) {$\beta$};
                \node at(-3,-4) (1) {1};
                \node at(6,-4) (T) {T};
                \node at(-5,-2) (N) {N};
                \node at(-5,3) (1state) {1};
                \node at(1.5,-4.5) (obs) {Observations};
                \node at(-6, 0.5) (State) {States};

                \node at(1.5,4) (gam) {$\gamma_t(i)$};

                \draw [draw=black, line width = 0.5mm] (-0.5,3.5) rectangle (3.5,-2.5);

                \draw (1) -- (T);
                \draw (1state) -- (N);
                \draw [->] (a11) -- (a21);
                \draw [->] (a11) -- (a22);
                \draw [->] (a11) -- (a2n);
                \draw [->] (a12) -- (a21);
                \draw [->] (a12) -- (a22);
                \draw [->] (a12) -- (a2n);
                \draw [->] (a1n) -- (a21);
                \draw [->] (a1n) -- (a22);
                \draw [->] (a1n) -- (a2n);
                \draw [->, dashed] (a21) -- (a31);
                \draw [->, dashed] (a21) -- (a32);
                \draw [->, dashed] (a21) -- (a3n);
                \draw [->, dashed] (a22) -- (a31);
                \draw [->, dashed] (a22) -- (a32);
                \draw [->, dashed] (a22) -- (a3n);
                \draw [->, dashed] (a2n) -- (a31);
                \draw [->, dashed] (a2n) -- (a32);
                \draw [->, dashed] (a2n) -- (a3n);
                \draw [->] (a31) -- (a41);
                \draw [->] (a31) -- (a42);
                \draw [->] (a31) -- (a4n);
                \draw [->] (a32) -- (a41);
                \draw [->] (a32) -- (a42);
                \draw [->] (a32) -- (a4n);
                \draw [->] (a3n) -- (a41);
                \draw [->] (a3n) -- (a42);
                \draw [->] (a3n) -- (a4n);
            \end{tikzpicture}
            \label{Hidden_Markov:Decoding:Figure_Gamma}
            \caption{Each dashed line represents a calculation for $\gamma_t(i)$. For each time step, we calculate these and then find the maximum. We select the state corresponding to the maximum and add it to the state sequence.}
        \end{center}
    \end{figure}
        
    As such this ensures all $\gamma_t(i)$ are probabilities. We can now go through each timestep $t$ calculating $\gamma_t(i)$ and selecting state $i$ corresponding to the largest $\gamma_t(i)$.


    This method will maximise the number of correct states and give the correct answer if the model is completely connected, i.e. each state can reach every other state. If this is not the case, we may get a state sequence that is not possible. e.g. At time $t$ the model is in state $i$ and at time $t+1$ it is in $j$, but $a_{ij} = 0$, thus is not possible.

    In the scenario given, we must redefine 'best' to the path that maximises $\prob (Q | O, \lambda)$. Since we are maximising this, we can equivalently maximise $\prob (Q, O | \lambda)$. To solve this problem, we require the Viterbi Algorithm.

    \subsection{Viterbi Algorithm}
    \label{Hidden_Markov:Decoding:Vierbi_Algorithm}

    In this subsection we will demonstrate the application of the Viterbi Algorithm.

    The Viterbi Algorithm was developed in \cite{Viterbi1967} and explored in further depth by in \cite{Forney1973}. It follows a similar lattice structure of the helper functions in the FB algorithm \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm}. Similar to how we broke \ref{Hidden_Markov:Using_HMM:Key_Problems:Questions:First} into smaller pieces, we do the same to $\prob (Q | O, \lambda)$.

    \begin{equation}
        \delta_t(i)  = max_{q_1,q_2,...,q_t-1} \prob (\{q_1,q_2,...q_t=i\},{O_1,O_2,...,O_t}|\lambda) 
    \end{equation}

    An important point to note is $\delta_t(i)$ does not store the sequence. Thus we must introduce a new variable that will be responsible for doing so. We can call this $\psi_t(i)$, where it is equal to the state we have come from given we are at time $t$ and state $i$. We will once again use induction. The process is as follows.

    \begin{enumerate}
        \item Base case \\
        For each state we must calculate $\pi_i b_i(O_1)$ to determine which intial state is most likely. We also must set $\psi_1(i)$ = 0 as there have not been any states until now. 
        \begin{eqnarray}
            \delta_1(i) & = & \pi_i b_i(O_1) \\
            \psi_1(i)   & = & 0
        \end{eqnarray}

        \item Inductive step: \\
        For each state we calculate the $\delta_{t-1}(i)$ times $a_{ij}$ to find the most likely next state based on previous state liklihood and the transition probability. We maximize this term  and the multiply by  $b_j(O_t)$ to include a bias based on the observation probabilities. As before we store the argument of the maximum in $\psi$. This gives us:
        \begin{eqnarray}
            \delta_t(j) & = & max_{1 \leq i \leq N}[\delta_{t-1}(i) a_{ij}] b_j(O_t),  t \in [2,T] \\
            \psi_t(j)   & = & argmax_{1 \leq i \leq N}[\delta_{t-1}(i) a_{ij}], j \in [1,N]
        \end{eqnarray}
        
        \begin{figure}
            \begin{center}
                \begin{tikzpicture}
                    \node[state] at(-3,3) (a11) {$\delta_{t}(j)$};
                    \node[state] at(-3,1)  (a12) {$\delta_{t}(j)$};
                    \node at(-3,0)  (sd1) {.};
                    \node at(-3,-0.5)  (sd2) {.};
                    \node at(-3,-1) (sd3) {.};
                    \node[state] at(-3,-2)  (a1n) {$\delta_{t}(j)$};
                    
                    \node[state] at(0,3) (a21) {$\delta_{t}(j)$};
                    \node[state] at(0,1)  (a22) {$\delta_{t}(j)$};
                    \node at(0,0)  (sd12) {.};
                    \node at(0,-0.5)  (sd22) {.};
                    \node at(0,-1) (sd32) {.};
                    \node[state] at(0,-2)  (a2n) {$\delta_{t}(j)$};
                    \node[state] at(3,3) (a31) {$\delta_{t}(j)$};
                    \node[state] at(3,1)  (a32) {$\delta_{t}(j)$};
                    \node at(3,0)  (sd13) {.};
                    \node at(3,-0.5)  (sd23) {.};
                    \node at(3,-1) (sd33) {.};
                    \node[state] at(3,-2)  (a3n) {$\delta_{t}(j)$};
                    
                    \node[state] at(6,3) (a41) {$\delta_{t}(j)$};
                    \node[state] at(6,1)  (a42) {$\delta_{t}(j)$};
                    \node at(6,0)  (sd14) {.};
                    \node at(6,-0.5)  (sd24) {.};
                    \node at(6,-1) (sd34) {.};
                    \node[state] at(6,-2)  (a4n) {$\delta_{t}(j)$};
                    \node at(-3,-4) (1) {1};
                    \node at(6,-4) (T) {T};
                    \node at(-5,-2) (N) {N};
                    \node at(-5,3) (1state) {1};
                    \node at(1.5,-4.5) (obs) {Observations};
                    \node at(-6, 0.5) (State) {States};
                    \draw (1) -- (T);
                    \draw (1state) -- (N);

                    \draw [draw=black, line width = 0.5mm] (-4,4) rectangle (-2,-3);

                \end{tikzpicture}
                \label{Hidden_Markov:Decoding:Gamma_Lattice}
                \caption{Once $\delta_{t}(j)$ is calculated for a particular $t$, we find the maximum, which is the state we add to our state sequence. Since $\delta_{t}(j)$ accounts for transition probability $a_{ij}$, impossible transitions will always be equal to 0. Thus unless all transitions are equal to 0, they cannot be the maximum.}
            \end{center}
        \end{figure}

        \item Termination step: \\
        To terminate this recursion, we need to find the maximum $\delta_{T}(i)$. Here we maximise, as the values we already calculated the values in the induction. We will store this probability in $\prob^*$. Similarly, we need to find the argument corresponding to this max for the final state, and we set this to the final state $q_T^*$.
        \begin{eqnarray}
            \prob^* & = & max_{1 \leq i \leq N}[\delta_{T}(i)] \\
            q_T^*   & = & argmax_{1 \leq i \leq N}[\delta_{T}(i)]
        \end{eqnarray}
        To find any particular states that we may be interseted in $q_t^*$, we use $q_t^*$ = $\psi_{t+1}(q_{t+1}^*)$. 
    \end{enumerate}

\section{Problem 3: Learning}
\label{Hidden_Markov:Learning}

HMMs have a large number of parameters. Obtaining these is a high-dimensional optimisation problem. In this section we utilise the Baum-Welch Algorithm to simulatenously solve for all HMM parameters.

The learning problem \ref{Hidden_Markov:Using_HMM:Key_Problems:Questions:Third} addresses how can we learn a model from given sequence of observations?  Given the sequence of observations $O$ = $\{O_1,O_2,...,O_T\}$ we want to find the most suitable $\lambda$ = $(A,B,\pi)$.


    \subsection{Baum-Welch Algorithm}
    \label{Hidden_Markov:Learning:Baum_Welch}

    In this subsection we will demonstrate the Baum-Welch Algorithm. Explanations have been inspired by \cite{Baum}.

    For the Baum-Welch algorithm we will need all three parameters intrudced earlier; $\alpha$ \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha}, $\beta$ \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Beta} and $\gamma$ \ref{Hidden_Markov:Decoding:gamma}. We will also need a new parameter $\xi_t(i,j)$. This will capture the probability of being in some state $S_i$ at time $t$ and then $S_j$ at time $t+1$ given the observations and the model. 
    \begin{equation}
        \xi_t(i,j) = \prob (q_t = S_i, q_{t+1} = S_j | O, \lambda)
    \end{equation} 

    To solve this problem we can refer back to \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha} for the left hand side (including $q_t$) and \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Beta} for the right hand side (indlucing $q_{t+1}$). To get from state $i$ to $j$ we use $a_{ij} b_j(O_{t+1})$. Putting this all togther we get a liklihood. To normalize this into a probabiltiy we once more use $\prob (O|\lambda)$. This gives us:
    \begin{equation}
        \xi_t(i,j) = \dfrac{\alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}{\prob (O|\lambda)}
    \end{equation}
    Since we are interested in the probability of going from one $i$ to one $j$, for the denominator we use the sum of all $i$ and all $j$. 
    \begin{equation}
        \xi_t(i,j) = \dfrac{\alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}
    \end{equation}

    One may find it helpful to compare $\xi_t(i,j)$ with $\gamma_t(i)$. $\gamma_t(i)$ represents the probability of being at state $i$ at timestep $t$. Summing over $j$ for $\xi_t(i,j)$ leaves us with $\xi_t(i)$ which represents the probability of being at state $i$ at timestep $t$. This is identical to $\gamma_t(i)$. We can now state: 

    \begin{equation}
        \gamma_t(i) = \sum_{j=1}^N \xi_t(i,j)
    \end{equation}

    For $\gamma_t(i)$, if we sum over $t$ we can get a number that can be used as the expected number of times $S_i$ is visited. Thus the expected number of transitions from $S_i$ can be calculated by:
    \begin{equation}
        \sum_{t=1}^{T-1} \gamma_t(i)
    \end{equation}

    Similarly for $\xi_t(i)$, if we sum over $t$ we can get a number that can be used as the expected number of times $S_i$ transitions to $S_j$.Thus the expected number of transitions from $S_i$ to $S_j$ can be calculated by:
    \begin{equation}
        \sum_{t=1}^{T-1} \xi_t(i,j)
    \end{equation}



    Now that we have created a set of tools, we will need to tackle the learning problem itself. To build a improved $\lambda$, $\bar{\lambda}$, we need to find $\bar{\pi}$, $\bar{A}$ and $\bar{B}$.

    \begin{enumerate}[i]
        \label{Hidden_Markov:Learning:Baum_Welch:Reestimation}
        \item $\bar{\pi}$ 
        
        Since $\gamma_t(i)$ represents the expected frequency in $S_i$ at time $t$, if we let $t$=1 this now is equvialent to $\bar{\pi}$
        \begin{equation}
            \bar{\pi} = \gamma_1(i)
        \end{equation}

        \item $\bar{a_{ij}}$
        
        Here we can use the expected number of transitions from state $i$ to $j$ divided by expected number of transitions from $i$ in total. This will provide the appropriate transition probability. 
        \begin{equation}
            \bar{a_{ij}} = \dfrac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
        \end{equation}

        \item $\bar{b_j(k)}$
        
        For the B matrix, we will need to divide the expected number of times the model is in state $j$ and observes $v_k$ by the expected number of times it is $j$.
        \begin{equation}
            \bar{b_j(k)} = \dfrac{\sum_{t=1, s.t. O_t = v_k}^T \gamma_t(j)}{\sum_{t=1}^{T-1} \gamma_t(j)}
        \end{equation} 
    \end{enumerate}

    We have used our $\lambda$ and $O$ to produce our parameters: $\alpha_t(i)$, $\beta_t(i)$, $\gamma_t(i)$ and $\xi_t(i,j)$. This is called parameter re-estimation. We have then used these parameters to produce our $\bar{\lambda}$ = $\{\bar{\pi}, \bar{a_{ij}}, \bar{b_j(k)} \}$. This is called expectation maximization. This method can be iterated and each iteration should provide an improvement for lambda or worst case senario be equvialent. Proof of this can be found in \cite{Baum}.  Once lambda stabalizes and is no longer changing, we can assume a local optimumum has been reached. 

    \begin{example}
        Let us continue from Example \ref{Hidden_Markov:Using_HMM:Simulation:Example}. Alice is unsure of her model, thus records Bob's mood for five days. Her Observation sequence is \{Happy, Sad, Happy, Sad, Sad\}. She wants to use one iteration of Baum-Welch to improve her model. Code files are available in the "Example" Folder under "MyCode".

        To apply the algorithm we need our $\alpha_t(i)$, $\beta_t(i)$, $\gamma_t(i)$ and $\xi_t(i,j)$. 

        We begin with $\alpha_t(i)$. We are expecting a lattice of size 3x5. We calculate using \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Alpha} and store the values in the following matrix:
        \[
            \begin{bmatrix}
                0.4  &  0.0494  & 0.048984 & 0.00912654 &  0.00250527 \\ 
                0.21 &  0.0849  & 0.059661 & 0.0173532  &  0.00540931 \\
                0.12 &  0.08    & 0.040704 & 0.018349   & 0.00570851  
            \end{bmatrix}
        \]
        
        Similarly for $\beta_t(i)$ using \ref{Hidden_Markov:Evaluation:Forward_Backward_Algorithm:Beta},
        \[
            \begin{bmatrix}
                0.0178115 & 0.0648   & 0.0844 & 0.28  & 1 \\
                0.0197177 & 0.062835 & 0.0943 & 0.31 &  1 \\
                0.0196482 & 0.063591 & 0.0949 & 0.31 & 1 
            \end{bmatrix}
        \]
        
        Using \ref{Hidden_Markov:Decoding:gamma} to calculate $\gamma_t(i)$,
        \[
            \begin{bmatrix}
                0.522979 & 0.234978 & 0.303474 & 0.187581 &  0.183899 \\ 
                0.303949 & 0.391592 & 0.412978 & 0.39488  & 0.397069  \\
                0.173072 & 0.373431 & 0.283549 & 0.417539 &  0.419032 
            \end{bmatrix}
        \]

        $\xi_t(i,j)$ produces a result which ressembles a transition matrix for each timestep. We will have 5-1 = 4 of these.

        \begin{itemize}
            \item \[
                \begin{bmatrix}
                    0.152212  & 0.221395  & 0.149372  \\
                    0.0599335 & 0.0871742 &  0.156841 \\ 
                    0.0228318 & 0.083023  & 0.0672175 
                \end{bmatrix}
            \]
            \item \[
                \begin{bmatrix}
                    0.0979363 & 0.0957461 &  0.0412951 \\
                    0.126237  & 0.123414  & 0.141942 \\
                    0.0793006 & 0.193818  & 0.100312 
                \end{bmatrix}
            \]           
            \item \[
                \begin{bmatrix}
                    0.0805428 &  0.133759 &  0.0891724 \\
                    0.073574  & 0.122185  & 0.217218 \\
                    0.0334641 &  0.138936 &  0.111149                      
                \end{bmatrix}
            \]
            \item \[
                \begin{bmatrix}
                    0.0535945 & 0.0803918 &  0.0535945 \\
                    0.0764283 & 0.114643  & 0.203809 \\
                    0.0538761 & 0.202035  & 0.161628 
                \end{bmatrix}
            \]
        \end{itemize}

        Using these along with \ref{Hidden_Markov:Learning:Baum_Welch:Reestimation} will allow us to re-estimate our A,B and $\pi$ parameters.

        $\pi$ is the first column of the $\gamma$ matrix. Thus our new inital distribution is 
        \begin{equation}
            \bar{\pi} = \{0.522979,	0.303949,	0.173072\}
        \end{equation}
        
        The number of transtions from state $i$ to $j$ can be taken as the sum of a particular position over all $\eta$ matrices. These can be divded by the sum of the corresponding $\gamma$ row, disregarding the last timestep.
        \begin{equation}
            \bar{a_{ij}} = \begin{bmatrix}
                0.307672  &	0.425369  &	0.266959 \\
                0.223609  &	0.297603  &	0.478789 \\
                0.151871  &	0.495204  &	0.352925
            \end{bmatrix}
        \end{equation}

        
        For each row of $\gamma$, we sum the values where the observation is Happy and where it is Sad and then divide both by the sum of the row. This gives us a row of our new observation matrix.
        \begin{equation}
            \bar{b_j(k)} = \begin{bmatrix}
                0.576765 & 0.423235 \\
                0.377237 & 0.622763 \\
                0.27398 & 0.72602
            \end{bmatrix}
        \end{equation} 

        This method can be repeated until convergence to retrieve the optimal parameters (local optimum).
    \end{example}

