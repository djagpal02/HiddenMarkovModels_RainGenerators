\section{Definition}

A hidden Markov model is a doubly stochastic process, where the underlying process is markovian and hidden \cite{1165342}, which comes from the fact that there are two stochastic processes, one determining the transition between states and one determining the output observation. Braum and his colleagues developed this in \cite{baum1967inequality} and \cite{baum1966}. 

To define a hidden Markov model, we need five things.
\begin{enumerate}
    \item N \begin{enumerate}[i]
        \item N is the number of hidden states. Usually based on something in the real world but sometimes can be unknown, as in \ref{motivhmmcoin}.
        \item The states are usually ergodic, i.e. from any given state, we can eventually reach another, but this is unnecessary.
        \item The states are from the state space $S = \{S_1,S_2,...,S_N\}$.
        \end{enumerate}
    \item M \begin{enumerate}[i]
        \item M is the number of observable outputs.
        \item These make a discrete alphabet of observations called $V = \{v_1, v_2,...,v_M\}$.
    \end{enumerate}
    \item A \begin{enumerate}[i]
        \item A is the state transition matrix.
        \item This is the same as the $p$ matrix \ref{pmat}. 
        \item A = $\{a_{ij}\}$ 
        \item $a_{ij}$ = $\{ p(i,j) = \prob \{ X_n = j | X_{n-1}\}$
    \end{enumerate}
    \item B \begin{enumerate}[i]
        \item B is the observation probability matrix.
        \item B = $\{b_j(k)\}$
        \item $b_j(k) = \prob (V_k \  at \  t | q_t = S_j)_{1 \leq j \leq N, 1 \leq k \leq M}$
    \end{enumerate}
    \item $\pi$ \begin{enumerate}[i]
        \item $\pi$ is a vector containing all inital state probabilities. 
        \item $\pi$ = $\{ \pi_i \}$
        \item $\pi_i$ = $\prob (q_1 = S_i) for 1 \leq i \leq N$
    \end{enumerate}
\end{enumerate}

We can now combine the above to provide a formal definition of hidden markov models.
\begin{definition}
\label{hmm} Hidden Markov Model \\
A Hidden Markov Model is a 5-tuple $\{N,M,A,B,\pi \}$ that is used to represent a doubly stochastic process where the hidden process is markovian. 
\end{definition}

Before we continue, we will also provide some notation used for the rest of the paper.
\begin{enumerate}[i]
    \item We will continue to use $Q$ \ref{stateseq} to represent the Markov model's state sequence, i.e. the hidden process.
    \item We will use $O = \{O_1,O_2,...,O_T\} ,\ O_i \in V$ to represent the observation sequence.
    \item we will occasionally represent the hidden markov model as $\{N,M, \lambda \}$ where $\lambda = \{A, B, \pi\}$ 
\end{enumerate}


\section{Using HMM}
As with any mathematical model, we can use HMMs to predict future observations. As with simple Markov models \ref{probmeas}, we can create probability measures, but this time we must use conditioning. 

\subsection{Predictive Model}
 We can use HMM to generate a sequence of potential observations. Given a a hidden markov model, lets call it H = $\{N,M,A,B,\pi\}$, to generate a sequence of $T$ observations $O$ we do the following steps:
 \begin{enumerate}
    \label{hmmpredict}
     \item Using $\pi$ as a probability measure, set $t$ = 1 and randomly select a state as the first $q_1$ = $s_i$.
     \item Using $b_i(k)$ as a probability measure, randomly select the observation $O_t$ = $v_k$.
     \item Using $a_{ij}$ as a probability measures, set $t$ = $t+1$ and randomly select a the next state $q_{t+1}$ = $s_j$.
     \item Repeat steps 2 and 3 until $t$ = $T$
 \end{enumerate}

 To demonstrate this method, we will use an adapted version of \ref{weathermarkovchain}. This version will have the Markov chain from before as the underlying hidden process and another process with state-space \{Happy, Sad\}, which we can observe.

 \begin{example}
    \label{motivationhmm}
    Suppose Alice is hidden away from the world and has no access to information regarding the weather. She meets Bob every day and knows how weather affects his mood.
    For simplicity, assume Bob only has two moods, happy and sad. Given matrix A, B and vector $\pi$ can she predict his mood for three consecutive days?
    \begin{figure}
        \centering
        \begin{tikzpicture}
            \node[state] at(-3,0) (s) {Sunny};
            \node[state] at(3,0)  (r) {Rainy};
            \node[state] at(0,-3)  (c) {Cloudy};
            \node at (-6,3) (h1) {Happy};
            \node at (-6,-3) (s1) {Sad};
            \node at (3,-6) (h2) {Happy};
            \node at (-3,-6) (s2) {Sad};
            \node at (6,3) (h3) {Happy};
            \node at (6,-3) (s3) {Sad};

            \draw[every loop]
                (s) edge[bend right=20, auto=left] node {0.4} (r)
                (s) edge[bend right=20, auto=right] node {0.2} (c)
                (s) edge[loop left] node {0.4} (s)       
                (r) edge[bend right=20, auto=left] node {0.4} (c)
                (r) edge[bend right=20, auto=right] node {0.3} (s)
                (r) edge[loop right] node {0.3} (r) 
                (c) edge[bend right=20, auto=right] node {0.5} (r)
                (c) edge[bend right=20, auto=left] node {0.2} (s)
                (c) edge[loop below] node {0.3} (c);
            
            \draw [->, dashed] (s) edge[auto=right] node{0.2} (s1);
            \draw [->, dashed] (s) edge[auto=right] node{0.8} (h1);
            \draw [->, dashed] (c) edge[auto=right] node{0.3} (s2);
            \draw [->, dashed] (c) edge[auto=right] node{0.6} (h2);
            \draw [->, dashed] (r) edge[auto=right] node{0.3} (s3);
            \draw [->, dashed] (r) edge[auto=right] node{0.7} (h3);
        \end{tikzpicture}
        \\
        The solid lines represent hidden probabilities, and the dashed represent observable.
    \end{figure}
   \\
    From context we can deduce the following:
    \begin*{enumerate}
        \begin{center}
        \item N = 3, number of hidden states
        \item M = 2, number of possible observations
        \item \begin{equation}
            A = 
            \begin{bmatrix}
                0.4 & 0.4 & 0.2 \\
                0.3 & 0.3 & 0.4 \\
                0.2 & 0.5 & 0.3 
                \end{bmatrix}
            \end{equation}
        \item \begin{equation}
            B = 
            \begin{bmatrix}
                0.8 & 0.2 \\
                0.7 & 0.3 \\
                0.6 & 0.4 
                \end{bmatrix}
            \end{equation}
        \item \begin{equation}
            \pi = 
            \begin{bmatrix}
                0.5 \\
                0.3 \\
                0.2 
                \end{bmatrix}
            \end{equation}
        \end{center}
    \end*{enumerate}

    We can now use \ref{hmmpredict} to create an observation sequence $O$ = $\{o_1, o_2, o_3\}$. We will require multiple random variables generated using various probability measures. We will these through python using the "rand.py" file. We will use a uniform random variable. We will label this r.v. We select the state that corresponds to the region on the cumulative probability distribution that the random variable lies on.
    \begin{enumerate}[i]
        \item We generate a r.v. = 0.0058. Using Pi as the probaility distribution, we select Sunny. We can now set $q_1$ = $s_1$.
        \item We generate a r.v. = 0.1947. Using $b_1(k)$ as the probability distribution we select Happy as the observation. We can now set $o_1$ = $v_1$.
        \item we generate a r.v. = 0.7168. Using $a_{1j}$ as the probability distribution we select Rainy as the next state. We now set $q_2$ = $s_2$.
        \item We generate a r.v. = 0.1060. Using $b_2(k)$ as the probability distribution we select Happy as the observation. We can now set $o_2$ = $v_1$.
        \item we generate a r.v. = 0.8977. Using $a_{2j}$ as the probability distribution we select Cloudy as the next state. We now set $q_3$ = $s_3$.
        \item We generate a r.v. = 0.1369. Using $b_3(k)$ as the probability distribution we select Happy as the observation. We can now set $o_2$ = $v_1$.
    \end{enumerate}
    Finally, we can look back on our prediction $O$ and see that it is equal to $\{v_1,v_1,v_1\}$, i.e. we predict three consecutive Happy days.
\end{example}


\subsection{Three Key Problems}
There are many interesting questions one may pose regarding the HMM but there are three famous ones, highlighted in \cite{1165342} which we will focus on. 
\begin{enumerate}
    \item \label{q:first} Evaluation \\ Given model H = $\{N,M,A,B,\pi\}$ what is the probability that it generated the sequence of observations $O = \{o_1,o_2,...,o_T\}$? i.e. $\prob (O \ | \ H)$
    \item \label{q:second} Decoding \\ What sequence of states $Q = \{q_1, q_2,..., q_3\}$ best explains a sequence of observations $O = \{o_1,o_2,...,o_T\}$?
    \item \label{q:third} Learning \\ Given a set of observation $O = \{o_1,o_2,...,o_T\}$, how can we learn the model H = $\{N,M,A,B,\pi\}$ that would generate them?
\end{enumerate}

In the coming sections, we will be motivating uses and developing solutions for each problem. 

\section{Problem 1: Evaluation}
Lets start by addressing question \ref{q:first}. Informally, we are looking for the probability that a given model generated a sequence of observations, i.e. $\prob (O | \lambda)$.

This probability has many useful applications. For example, we may have multiple potential models $\lambda_i$ and cannot decide which one is the most suitable. In this case, we can now calculate this probability for each $\lambda_i$ and the largest.

To find this probability, we must consider the hidden internal states of the model. Since our probability of observations $\{b_j(k)\}$ is conditioned on the hidden state, we can start by calculating this probability conditioned on these states. Lets assume we know what the state sequence $Q = \{q_1,q_2,...,q_t\}$ is. To $\prob (O | Q, \lambda)$ we can find the product of all the probabilities of an observation given the models state at all times $t$. In essence, this breaks the  $\prob (O | Q, \lambda)$ into T parts. 
\begin{equation}
    \label{poq1}
    \prob (O | Q, \lambda) = \prod_{t=1}^T \prob (O_t | q_t, \lambda)
\end{equation}


An observation one may make is that these probabilities are simply taken from the matrix $B$. 
\begin{equation}
    \prob (O_t | q_t, \lambda) = b_{q_t}(O_t), \ \ \forall t \in [0,T] 
\end{equation}

Thus we can rewrite \ref{poq1} as:
\begin{equation}
    \label{oqlam}
    \prob (O | Q, \lambda) = b_{q_1}(O_1) b_{q_2}(O_2)  ...  b_{q_T}(O_T)
\end{equation}

Our next objective is to remove $Q$ from the conditioned part of the probability. To do this, we must first calculate $\prob (Q | \lambda)$. This is simply the probability of transitioning from $q_1$ to $q_2$, $q_2$ to $q_3$ etc. More formally we can use matrix $A$ to find the probability of each of these transitions, and since we are finding the total for the entire sequence, we multiply them all together. We start with $\pi_{q_1}$ as we also need the probability of starting at $q_1$.
\begin{equation}
    \label{qlam}
    \prob ( Q | \lambda) = \pi_{q_1} a_{q_1q_2} a_{q_2q_3} ... a_{q_{T-1}q_T}
\end{equation}

We can now successfully remove $Q$ from the condition using \ref{oqlam} and \ref{qlam}:
\begin{eqnarray}
    \label{oqgivenlam}
    \prob (O,Q | \lambda) & = & \prob(O | Q, \lambda) \prob(Q | \lambda) \\
                          & = & \pi_{q_1} b_{q_1}(O_1) a_{q_1q_2} b_{q_2}(O_2) a_{q_2q_3} ... a_{q_{T-1}q_T} b_{q_T}(O_T)
\end{eqnarray}

This gives us the joint probability of observations and the internal states. In other words, it provides the probabilty that given observations $O$ and internal state sequence $Q$ was generated by model $\lambda$. To achieve our desired probablity all we need to do is get rid of the $Q$. Since it is another input, all we must do is sum each value of \ref{oqgivenlam}. As we have accounted for every possible $Q$, we no longer need to worry about its particular value. This leaves us with:
\begin{eqnarray}
    \label{ogivenlamsumq}
    \prob (O | \lambda) & = & \sum_{all Q} \prob(O| Q,\lambda)P(Q|\lambda) \\
                        & = & \sum_{q_1,q_2,...,q_T} \pi_{q_1} b_{q_1}(O_1) a_{q_1q_2} b_{q_2}(O_2) a_{q_2q_3} ... a_{q_{T-1}q_T} b_{q_T}(O_T)
\end{eqnarray} 

Although this solution is correct, calculating this is infeasible because it requires too many computations. For $T$ timesteps and $N$ states, to find every possible $Q$, we must sum over $N^T$ state sequences. For each timestep we require a multiplication to $a_{q_{i-1}q_{i}}$ and $b_{q_i}(O_i)$, except the last where there are no transitions, which leads to $2T-1$ multiplications for each state sequence. Lastly, we require $N^T$ addition operations to sum the result for each state sequence. Our final total number of operations of $(2T-1)N^T + (N^T-1)$. 

\begin{example}
    \label{motivfb}
    Lets refer back to \ref{motivationhmm}. Suppose we would like to find the probability of a string of 10 observations using model H. Here $N$ = 3 and $T$ = 10. 
    \begin{eqnarray}
        operations & = & (2T-1)N^T + (N^T-1) \\
                   & = & (20-1)3^10 + (3^10 -1) \\
                   & = & 1180979
    \end{eqnarray}
\end{example}


We have a problem as even with a smaller model \ref{motivfb} we require a considerable number of calculations. For a moderate to large-sized model, we would require an infeasible amount of calculations. To overcome this problem, we look for a more efficient method, the Forward-Backward algorithm.



\subsection{Forward-Backward Algorithm}
The Forward-Backward algorithm (F-B) comprises two helper functions $\alpha$ and $\beta$. We will start by discussing the former.

\begin{equation}
    \label{alpha}
    \alpha_t(i) = \prob (O_1,O_2,O_3,...,O_t,q_t=S_i | \lambda)
\end{equation}
$\alpha$ is an extremly powerful tool in reducing the number of calculations. As given by \ref{alpha}, it providse the probability that at time $t$ we have seen a sequence of observations and are currently at state $q_t=S_i$. This is not quite $\prob (O | \lambda)$ but it represents a part of it. Instead of the probability of the whole sequence, it breaks it into a chunk of size $t$, commits to end at a particular state and then calculates the same proabbility for this.

We can combine \label{alpha} with induction to produce an iterative process that can calculate \ref{q: first}.

\begin{enumerate}[i]
    \item Base case \\
    For the base case we require the probability of the $q_1$ being equal to $S_1$ and giving us observation $O_1$. The former is addressed by $\pi_i$ and the later by $b_i(O_1)$. This gives us:
    \begin{eqnarray}
        \alpha_1(i) = \pi_i b_i(O_1), & i \in [1,N]
    \end{eqnarray}

    \item Inductive step: \\
    \begin{center}
        \begin{tikzpicture}
            \node[state] at(-2,3) (s1) {$S_1$};
            \node[state] at(-2,1)  (s2) {$S_2$};
            \node at(-2,0)  (sd1) {.};
            \node at(-2,-0.5)  (sd2) {.};
            \node at(-2,-1) (sd3) {.};
            \node[state] at(-2,-2)  (sn) {$S_N$};
            \node[state] at(3,0) (new) {$S_j$};
            \node at(-2,-3) (a1) {$\alpha_t(i)$};
            \node at(3,-3) (a2) {$\alpha_{t+1}(i)$};
            \node at(-2,-4) (t1) {$t$};
            \node at(3,-4) (t2) {$t+1$};

            \draw [->] (s1) edge[auto=right] node{$a_{1j}$} (new);
            \draw [->] (s2) edge[auto=right] node{$a_{2j}$} (new);
            \draw [->] (sn) edge[auto=right] node{$a_{Nj}$} (new);
        \end{tikzpicture}
        \\
        For each state $j$, for each inductive step, we follow the above method. Each$\alpha_t  (i)$ is multiplied by each $a_{ij}$ and then summed up. This resultmultipled by $b_j  (O_{t+1}$ gives us $\alpha_{t+1}(j)$.
    \end{center}

    For the inductive step we must consider how to approach the next timestep. We will again be calculating for all $j\in [1,N]$ and as such must take into consideration, for each $j$, every possible $i$. This is again the same set of $[1,N]$. Therefore, to account for all possible previous states and their transition to the current state, we must sum over 1 to $N$ the product of $\alpha_t(i)$ and $a_{ij}$. For the given observation, as before, we compute $b_j(O_{t+1})$. Additionally, we must stop before reaching the final step as there is no outward transition and thus this would not be applicable. This gives us:
    \begin{eqnarray}
        \alpha_{t+1}(j) = \left[\sum_{i=1}^N \alpha_t(i)a_{ij}\right] b_j(O_{t+1}), & j \in [1,N], t \in [1,T-1]
    \end{eqnarray}

    \item Termination step: \\
    For the termination step, we sum over all alpha at the final time $T$. Each alpha represents the probability of being in that given state at that particular time, through all possible sequences that it may have followed. 
    \begin{eqnarray}
        \prob (O | \lambda) = \sum_{i=1}^N \alpha_T(i)
    \end{eqnarray}
\end{enumerate}



\begin{center}
    \begin{tikzpicture}
        \node[state] at(-3,3) (a11) {$\alpha_1(1)$};
        \node[state] at(-3,1)  (a12) {$\alpha_1(2)$};
        \node at(-3,0)  (sd1) {.};
        \node at(-3,-0.5)  (sd2) {.};
        \node at(-3,-1) (sd3) {.};
        \node[state] at(-3,-2)  (a1n) {$\alpha_1(N)$};
        
        \node[state] at(0,3) (a21) {$\alpha_2(1)$};
        \node[state] at(0,1)  (a22) {$\alpha_2(2)$};
        \node at(0,0)  (sd12) {.};
        \node at(0,-0.5)  (sd22) {.};
        \node at(0,-1) (sd32) {.};
        \node[state] at(0,-2)  (a2n) {$\alpha_2(N)$};
        \node[state] at(3,3) (a31) {$\alpha_{T-1}(1)$};
        \node[state] at(3,1)  (a32) {$\alpha_{T-1}(2)$};
        \node at(3,0)  (sd13) {.};
        \node at(3,-0.5)  (sd23) {.};
        \node at(3,-1) (sd33) {.};
        \node[state] at(3,-2)  (a3n) {$\alpha_{T-1}(N)$};
        
        \node[state] at(6,3) (a41) {$\alpha_{T}(1)$};
        \node[state] at(6,1)  (a42) {$\alpha_{T}(2)$};
        \node at(6,0)  (sd14) {.};
        \node at(6,-0.5)  (sd24) {.};
        \node at(6,-1) (sd34) {.};
        \node[state] at(6,-2)  (a4n) {$\alpha_{T}(N)$};
        \node at(-3,-4) (1) {1};
        \node at(6,-4) (T) {T};
        \node at(-5,-2) (N) {N};
        \node at(-5,3) (1state) {1};
        \node at(1.5,-4.5) (obs) {Observations};
        \node at(-6, 0.5) (State) {States};
        \draw (1) -- (T);
        \draw (1state) -- (N);
        \draw [->] (a11) -- (a21);
        \draw [->] (a11) -- (a22);
        \draw [->] (a11) -- (a2n);
        \draw [->] (a12) -- (a21);
        \draw [->] (a12) -- (a22);
        \draw [->] (a12) -- (a2n);
        \draw [->] (a1n) -- (a21);
        \draw [->] (a1n) -- (a22);
        \draw [->] (a1n) -- (a2n);
        \draw [->, dashed] (a21) -- (a31);
        \draw [->, dashed] (a21) -- (a32);
        \draw [->, dashed] (a21) -- (a3n);
        \draw [->, dashed] (a22) -- (a31);
        \draw [->, dashed] (a22) -- (a32);
        \draw [->, dashed] (a22) -- (a3n);
        \draw [->, dashed] (a2n) -- (a31);
        \draw [->, dashed] (a2n) -- (a32);
        \draw [->, dashed] (a2n) -- (a3n);
        \draw [->] (a31) -- (a41);
        \draw [->] (a31) -- (a42);
        \draw [->] (a31) -- (a4n);
        \draw [->] (a32) -- (a41);
        \draw [->] (a32) -- (a42);
        \draw [->] (a32) -- (a4n);
        \draw [->] (a3n) -- (a41);
        \draw [->] (a3n) -- (a42);
        \draw [->] (a3n) -- (a4n);
    \end{tikzpicture}
    \\
    For each $\alpha$ at time $t$, we require all $\alpha$ at time $t-1$. Thus werequire $N^2$ calculations for each timestep.
\end{center}


One can see that this method is far more efficient than \ref{ogivenlamsumq}. It requires $N^2$ multiplications for  $\alpha_t(i)$ and $a_{ij}$  for $T$ time periods. At each $T$ there are $N$ addition operations for the summation and  $N$ multiplication for the $b_j(O_{t+1})$, which is true for all but the first and last timestep, where there are $N$ multiplications and $N$ additions respectively. This gives us $(T-2)(N^2 + N + 1) + 2N$ calculations. Considering the order, we see that the previous method had order $O(TN^T)$ whereas F-B gives us an order of $O(TN^2)$. It is now feasible to compute $\prob (O | \lambda)$.

\begin{example}
    \label{motivfb2}
    We attempt to solve the problem \ref{motivfb} again but this time using the F-B algorithm. Suppose we would like to find the probability of a string of 10 observations using model H. Here $N$ = 3 and $T$ = 10. 
    \begin{eqnarray}
        operations & = & (T-2)(N^2 + N + 1) + 2N \\
                   & = & (10-2)(3^2 + 10 + 1) + 6\\
                   & = & 118
    \end{eqnarray}

    118 is many orders of magnitude smaller than 1180979, which was the requirement without F-B algorithm. Thus it is a significant improvement. The improvement against the base method will be even more drastic for larger models, as F-B has a fixed $N^2$ instead of the $N^T$ term.  
\end{example}


For question 1 this is sufficient. However, we will later require the $\beta$, the backward component and as such we will describe it now. Logically it is the same principle as the forward component except this time instead of moving forward step by step we are moving backwards.
\begin{equation}
    \label{beta}
    \beta_t(i) = \prob (O_{t+1},O_{t+2},,...,O_T | q_t=S_i, \lambda)
\end{equation}
This represents the probability of seeing the observations $O_{t+1}$ up to $O_T$ given that at time $t$ the model $\lambda$ is at state $S_i$. We again calculate this inductively.

\begin{enumerate}
    \item Base case \\
    For each state we must assume that it is the final state in order to iterate from it. This gives us:
    \begin{eqnarray}
        \beta_T(i) = 1, & i \in [1,N]
    \end{eqnarray}

    \item Inductive step: \\
    \begin{center}
        \begin{tikzpicture}
            \node[state] at(3,3) (s1) {$S_1$};
            \node[state] at(3,1)  (s2) {$S_2$};
            \node at(3,0)  (sd1) {.};
            \node at(3,-0.5)  (sd2) {.};
            \node at(3,-1) (sd3) {.};
            \node[state] at(3,-2)  (sn) {$S_N$};
            \node[state] at(-2,0) (new) {$S_j$};
            \node at(3,-3) (a1) {$\beta_{t+1}(i)$};
            \node at(-2,-3) (a2) {$\beta_{t}(i)$};
            \node at(3,-4) (t1) {$t+1$};
            \node at(-2,-4) (t2) {$t$};

            \draw [->] (new) edge[auto=right] node{$a_{1j}$} (s1);
            \draw [->] (new) edge[auto=right] node{$a_{2j}$} (s2);
            \draw [->] (new) edge[auto=right] node{$a_{Nj}$} (sn);
        \end{tikzpicture}
        \\
        Similar to $\alpha$, we use induction. However, this time we look into the future of the sequence and take steps backwards.
    \end{center}

    Each inductive step we move back by one timestep. As before, we will be calculating for all $N$ states except this time $i$ will be varying instead of $j$. This makes sense as we are stepping backward and we want to see which previous state is the most likely previous state. At each step, we the transaction probability of having coming from $i$, $a_{ij}$ to the probability of seeing the given observation at state $S_j$, $b_j(O_{t+1})$ and the liklihood of being at that state based on future states $\beta_{t+1}(i)$. This gives us:
    \begin{eqnarray}
        \beta_{t}(i) = \sum_{j=1}^N a_{ij} b_j(O_{t+1}) \beta_{t+1}(i), & i \in [1,N], t \in [1,T-1]
    \end{eqnarray}
\end{enumerate}



\section{Problem 2: Decoding}
We now turn our attention to \ref{q:second}. This problem is somewhat more difficult, as the definition of 'best' is quite vague and open to interpretations. An obvious appraoch would be to look for the states that are individually most likely for each state for a set time $t$ given all observations and the model. Lets define this probaility as $\gamma$.
\begin{equation}
    \label{gamma}
    \gamma_t(i) = \prob (q_t = S_i | O, \lambda)
\end{equation}

Using \ref{alpha} and \ref{beta} we can solve for $\gamma$ quite quickly. If we recall, $\alpha_t(i)$ provides us with the probability of being in state $i$ after seeing all the previous observations and $\beta_t(i)$ gives us the probability of being in state $i$ see all the remaining observations in the future. This gives us $\alpha_t(i) \beta_t(i)$. We now normalise this to get a probability, which is possible by dividing by the probability of getting this particular observation given all possible observation sequences $\prob (O|\lambda)$. This is equivalent to dividing by the sum of $\alpha_t(j) \beta_t(j)$ over all possible states $j$.
\begin{eqnarray}
    \gamma_t(i) & = &  \dfrac{\alpha_t(i) \beta_t(i)}{\prob (O|\lambda)} \\
                & = &  \dfrac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)}
\end{eqnarray}


This ensures that:
\begin{equation}
    \sum_{i=1}^N \gamma_t(i) = 1
\end{equation}

\begin{center}
    \begin{tikzpicture}
        \node[state] at(-3,3) (a11) {$\alpha$};
        \node[state] at(-3,1)  (a12) {$\alpha$};
        \node at(-3,0)  (sd1) {.};
        \node at(-3,-0.5)  (sd2) {.};
        \node at(-3,-1) (sd3) {.};
        \node[state] at(-3,-2)  (a1n) {$\alpha$};
        
        \node[state] at(0,3) (a21) {$\alpha$};
        \node[state] at(0,1)  (a22) {$\alpha$};
        \node at(0,0)  (sd12) {.};
        \node at(0,-0.5)  (sd22) {.};
        \node at(0,-1) (sd32) {.};
        \node[state] at(0,-2)  (a2n) {$\beta$};
        \node[state] at(3,3) (a31) {$\beta$};
        \node[state] at(3,1)  (a32) {$\beta$};
        \node at(3,0)  (sd13) {.};
        \node at(3,-0.5)  (sd23) {.};
        \node at(3,-1) (sd33) {.};
        \node[state] at(3,-2)  (a3n) {$\beta$};
        
        \node[state] at(6,3) (a41) {$\beta$};
        \node[state] at(6,1)  (a42) {$\beta$};
        \node at(6,0)  (sd14) {.};
        \node at(6,-0.5)  (sd24) {.};
        \node at(6,-1) (sd34) {.};
        \node[state] at(6,-2)  (a4n) {$\beta$};
        \node at(-3,-4) (1) {1};
        \node at(6,-4) (T) {T};
        \node at(-5,-2) (N) {N};
        \node at(-5,3) (1state) {1};
        \node at(1.5,-4.5) (obs) {Observations};
        \node at(-6, 0.5) (State) {States};

        \node at(1.5,4) (gam) {$\gamma_t(i)$};

        \draw [draw=black, line width = 0.5mm] (-0.5,3.5) rectangle (3.5,-2.5);

        \draw (1) -- (T);
        \draw (1state) -- (N);
        \draw [->] (a11) -- (a21);
        \draw [->] (a11) -- (a22);
        \draw [->] (a11) -- (a2n);
        \draw [->] (a12) -- (a21);
        \draw [->] (a12) -- (a22);
        \draw [->] (a12) -- (a2n);
        \draw [->] (a1n) -- (a21);
        \draw [->] (a1n) -- (a22);
        \draw [->] (a1n) -- (a2n);
        \draw [->, dashed] (a21) -- (a31);
        \draw [->, dashed] (a21) -- (a32);
        \draw [->, dashed] (a21) -- (a3n);
        \draw [->, dashed] (a22) -- (a31);
        \draw [->, dashed] (a22) -- (a32);
        \draw [->, dashed] (a22) -- (a3n);
        \draw [->, dashed] (a2n) -- (a31);
        \draw [->, dashed] (a2n) -- (a32);
        \draw [->, dashed] (a2n) -- (a3n);
        \draw [->] (a31) -- (a41);
        \draw [->] (a31) -- (a42);
        \draw [->] (a31) -- (a4n);
        \draw [->] (a32) -- (a41);
        \draw [->] (a32) -- (a42);
        \draw [->] (a32) -- (a4n);
        \draw [->] (a3n) -- (a41);
        \draw [->] (a3n) -- (a42);
        \draw [->] (a3n) -- (a4n);
    \end{tikzpicture}
    \\
    Each dashed line represents a calculation for $\gamma_t(i)$. For each time step, we calculate these and then find the maximum. We select the state corresponding to the maximum and add it to the state sequence.
\end{center}

As such this ensures all $\gamma_t(i)$ are probabilities. We can now go through each timestep $t$ calculating $\gamma_t(i)$ and selecting state $i$ corresponding to the largest $\gamma_t(i)$.


This method will maximise the number of correct states and give the correct answer if the model is completely connected, i.e. each state can reach every other state. If this is not the case, we may get a state sequence that is not possible. e.g. At time $t$ the model is in state $i$ and at time $t+1$ it is in $j$, but $a_{ij} = 0$, thus is not possible.

In the scenario given, we must redefine 'best' to the path that maximises $\prob (Q | O, \lambda)$. Since we are maximising this, we can equivalently maximise $\prob (Q, O | \lambda)$. To solve this problem, we require the Viterbi Algorithm.


\subsection{Viterbi Algorithm}
The Viterbi Algorithm \cite{viterbi1967error}, explored in depth in \cite{forney1973viterbi} follows a similar lattice structure of the F-B algorithm. Similar to how we broke \ref{q:first} into smaller pieces, we do the same to $\prob (Q | O, \lambda)$.

\begin{equation}
    \delta_t(i)  = max_{q_1,q_2,...,q_t-1} \prob (\{q_1,q_2,...q_t=i\},{O_1,O_2,...,O_t}|\lambda) 
\end{equation}

An important point to note is $\delta_t(i)$ does not store the sequence. Thus we must introduce a new variable that will be responsible for doing so. We can call this $\psi_t(i)$, where it is equal to the state we have come from given we are at time $t$ and state $i$. We will once again use induction. The process is as follows.

\begin{enumerate}
    \item Base case \\
    For each state we must calculate $\pi_i b_i(O_1)$ to determine which intial state is most likely. We also must set $\psi_1(i)$ = 0 as there have not been any states until now. 
    \begin{eqnarray}
        \delta_1(i) & = & \pi_i b_i(O_1) \\
        \psi_1(i)   & = & 0
    \end{eqnarray}

    \item Inductive step: \\
    For each state we calculate the $\delta_{t-1}(i)$ times $a_{ij}$ to find the most likely next state based on previous state liklihood and the transition probability. We maximize this term  and the multiply by  $b_j(O_t)$ to include a bias based on the observation probabilities. As before we store the argument of the maximum in $\psi$. This gives us:
    \begin{eqnarray}
        \delta_t(j) & = & max_{1 \leq i \leq N}[\delta_{t-1}(i) a_{ij}] b_j(O_t),  t \in [2,T] \\
        \psi_t(j)   & = & argmax_{1 \leq i \leq N}[\delta_{t-1}(i) a_{ij}], j \in [1,N]
    \end{eqnarray}
    \begin{center}
        \begin{tikzpicture}
            \node[state] at(-3,3) (a11) {$\delta_{t}(j)$};
            \node[state] at(-3,1)  (a12) {$\delta_{t}(j)$};
            \node at(-3,0)  (sd1) {.};
            \node at(-3,-0.5)  (sd2) {.};
            \node at(-3,-1) (sd3) {.};
            \node[state] at(-3,-2)  (a1n) {$\delta_{t}(j)$};
            
            \node[state] at(0,3) (a21) {$\delta_{t}(j)$};
            \node[state] at(0,1)  (a22) {$\delta_{t}(j)$};
            \node at(0,0)  (sd12) {.};
            \node at(0,-0.5)  (sd22) {.};
            \node at(0,-1) (sd32) {.};
            \node[state] at(0,-2)  (a2n) {$\delta_{t}(j)$};
            \node[state] at(3,3) (a31) {$\delta_{t}(j)$};
            \node[state] at(3,1)  (a32) {$\delta_{t}(j)$};
            \node at(3,0)  (sd13) {.};
            \node at(3,-0.5)  (sd23) {.};
            \node at(3,-1) (sd33) {.};
            \node[state] at(3,-2)  (a3n) {$\delta_{t}(j)$};
            
            \node[state] at(6,3) (a41) {$\delta_{t}(j)$};
            \node[state] at(6,1)  (a42) {$\delta_{t}(j)$};
            \node at(6,0)  (sd14) {.};
            \node at(6,-0.5)  (sd24) {.};
            \node at(6,-1) (sd34) {.};
            \node[state] at(6,-2)  (a4n) {$\delta_{t}(j)$};
            \node at(-3,-4) (1) {1};
            \node at(6,-4) (T) {T};
            \node at(-5,-2) (N) {N};
            \node at(-5,3) (1state) {1};
            \node at(1.5,-4.5) (obs) {Observations};
            \node at(-6, 0.5) (State) {States};
            \draw (1) -- (T);
            \draw (1state) -- (N);

            \draw [draw=black, line width = 0.5mm] (-4,4) rectangle (-2,-3);

        \end{tikzpicture}
        \\
        Once $\delta_{t}(j)$ is calculated for a particular $t$, we find the maximum, which is the state we add to our state sequence. Since $\delta_{t}(j)$ accounts for transition probability $a_{ij}$, impossible transitions will always be equal to 0. Thus unless all transitions are equal to 0, they cannot be the maximum.
    \end{center}

    \item Termination step: \\
    To terminate this recursion, we need to find the maximum $\delta_{T}(i)$. Here we maximise, as the values we already calculated the values in the induction. We will store this probability in $\prob*$. Similarly, we need to find the argument corresponding to this max for the final state, and we set this to the final state $q_T^*$.
    \begin{eqnarray}
        \prob^* & = & max_{1 \leq i \leq N}[\delta_{T}(i)] \\
        q_T^*   & = & argmax_{1 \leq i \leq N}[\delta_{T}(i)]
    \end{eqnarray}
    To find any particular states that we may be interseted in $q_t^*$, we use $q_t^*$ = $\psi_{t+1}(q_{t+1}^*)$. 
\end{enumerate}



\section{Problem 3: Learning}
The third problem \ref{q:third} addresses learning. How can we learn a model from given data of observations? This problem is critical as we often do not have the model, and calculating it is not simple. In these cases, we must derive the model from the data we have, a set of observations. Given the set of observations $O$ = $\{O_1,O_2,...,O_T\}$ we want to find the most suitable $\lambda$ = $(A,B,\pi)$. To do this, we use the Baum-Welch algorithm.


\subsection{Baum-Welch Algorithm}
For the Baum-Welch algorithm we will need all three parameters intrudced earlier; $\alpha$ \ref{alpha}, $\beta$ \ref{beta} and $\gamma$ \ref{gamma}. We will also need a new parameter $\xi_t(i,j)$. This will capture the probability of being in some state $S_i$ at time $t$ and then $S_j$ at time $t+1$ given the observations and the model. 
\begin{equation}
    \xi_t(i,j) = \prob (q_t = S_i, q_{t+1} = S_j | O, \lambda)
\end{equation} 

To solve this problem we can refer back to \ref{alpha} for the left hand side (including $q_t$) and \ref{beta} for the right hand side (indlucing $q_{t+1}$). To get from state $i$ to $j$ we use $a_{ij} b_j(O_{t+1})$. Putting this all togther we get a liklihood. To normalize this into a probabiltiy we once more use $\prob (O|\lambda)$. This gives us:
\begin{equation}
    \xi_t(i,j) = \dfrac{\alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}{\prob (O|\lambda)}
\end{equation}
Since we are interested in the probability of going from one $i$ to one $j$, for the denominator we use the sum of all $i$ and all $j$. 
\begin{equation}
    \xi_t(i,j) = \dfrac{\alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}
\end{equation}

One may find it helpful to compare $\xi_t(i,j)$ with $\gamma_t(i)$. $\gamma_t(i)$ represents the probability of being at state $i$ at timestep $t$. Summing over $j$ for $\xi_t(i,j)$ leaves us with $\xi_t(i)$ which represents the probability of being at state $i$ at timestep $t$. This is identical to $\gamma_t(i)$. We can now state: 

\begin{equation}
    \gamma_t(i) = \sum_{j=1}^N \xi_t(i,j)
\end{equation}

For $\gamma_t(i)$, if we sum over $t$ we can get a number that can be used as the expected number of times $S_i$ is visited. Thus the expected number of transitions from $S_i$ can be calculated by:
\begin{equation}
    \sum_{t=1}^{T-1} \gamma_t(i)
\end{equation}

Similarly for $\xi_t(i)$, if we sum over $t$ we can get a number that can be used as the expected number of times $S_i$ transitions to $S_j$.Thus the expected number of transitions from $S_i$ to $S_j$ can be calculated by:
\begin{equation}
    \sum_{t=1}^{T-1} \xi_t(i,j)
\end{equation}



Now that we have created a set of tools, we will need to tackle the learning problem itself. To build a improved $\lambda$, $\bar{\lambda}$, we need to find $\bar{\pi}$, $\bar{A}$ and $\bar{B}$.

\begin{enumerate}[i]
    \item $\bar{\pi}$ \\
    Since $\gamma_t(i)$ represents the expected frequency in $S_i$ at time $t$, if we let $t$=1 this now is equvialent to $\bar{\pi}$ \\
    \begin{equation}
        \bar{\pi} = \gamma_1(i)
    \end{equation}

    \item $\bar{a_{ij}}$ \\
    Here we can use the expected number of transitions from state $i$ to $j$ divided by expected number of transitions from $i$ in total. This will provide the appropriate transition probability. \\
    \begin{equation}
        \bar{a_{ij}} = \dfrac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
    \end{equation}

    \item $\bar{b_j(k)}$ \\
    For the B matrix, we will need to divide the expected number of times the model is in state $j$ and observes $v_k$ by the expected number of times it is $j$.
    \begin{equation}
        \bar{b_j(k)} = \dfrac{\sum_{t=1, s.t. O_t = v_k}^T \gamma_t(j)}{\sum_{t=1}^{T-1} \gamma_t(j)}
    \end{equation} 
\end{enumerate}

We have used our $\lambda$ and $O$ to produce our parameters: $\alpha_t(i)$, $beta_t(i)$, $\gamma_t(i)$, $\xi_t(i,j)$. This is called parameter re-estimation. We have then used these parameters to produce our $\bar{\lambda}$ = $\{\bar{\pi}, \bar{a_{ij}}, \bar{b_j(k)} \}$. This is called expectation maximization. This method can be iterated and each iteration should provide an improvement for lambda or worst case senario be equvialent. Proof of this can be found in \cite{baum1972inequality}.  Once lambda stabalizes and is no longer changing, we can assume a local optimumum has been reached. 



\section{Modified HMM}
\subsection{GMM}




